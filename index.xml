<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Dexiong Chen</title>
    <link>https://claying.github.io/</link>
    <description>Recent content in Home on Dexiong Chen</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://claying.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Use Lightning and Hydra to train your neural networks</title>
      <link>https://claying.github.io/blogs/2025-02-24-lightning-and-hydra/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://claying.github.io/blogs/2025-02-24-lightning-and-hydra/</guid>
      <description>&lt;!-- **TL;DR:** _A [tutorial][4] for training a Llama model on protein sequences from scratch with less than 500 lines of code._ --&gt;&#xA;&lt;p&gt;Deep learning research is moving at an incredible pace, and so are the tools that support rapid experimentation.&#xA;In this post, we&amp;rsquo;ll explore how to combine &lt;a href=&#34;https://lightning.ai/&#34;&gt;PyTorch Lightning&lt;/a&gt; and &lt;a href=&#34;https://hydra.cc/&#34;&gt;Hydra&lt;/a&gt; to streamline the training of a deep neural network—in our case,&#xA;a Llama model trained on protein sequences, which will allow for protein sequence generation via next amino acid prediction.&#xA;We’ll also integrate components from &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;Hugging Face&lt;/a&gt; to leverage state-of-the-art language model architectures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>https://claying.github.io/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://claying.github.io/publications/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.02216&#34;&gt;&lt;strong&gt;Flatten Graphs as Sequences: Transformers are Scalable Graph Generators&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Markus Krimmel, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;Preprint&lt;/em&gt;, 2025 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2502.02216&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2502.02415&#34;&gt;&lt;strong&gt;Towards Fast Graph Generation via Autoregressive Noisy Filtration Modeling&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Markus Krimmel, Jenna Wiens, Karsten Borgwardt, Dexiong Chen &lt;br /&gt;&#xA;&lt;em&gt;Preprint&lt;/em&gt;, 2025 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2502.02415&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.14819&#34;&gt;&lt;strong&gt;Endowing Protein Language Models with Structural Knowledge&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;Preprint&lt;/em&gt;, 2024 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2401.14819&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/PST&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Learning Laplacian Positional Encodings for Heterophilous Graphs&lt;/strong&gt; &lt;br /&gt;&#xA;Michael Ito, Jiong Zhu, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Danai Koutra, Jenna Wiens &lt;br /&gt;&#xA;&lt;em&gt;AISTATS&lt;/em&gt;, 2025 &lt;br /&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2406.03386&#34;&gt;&lt;strong&gt;Learning Long Range Dependencies on Graphs via Random Walks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Till Hendrik Schulz, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;ICLR&lt;/em&gt;, 2025 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2406.03386&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/NeuralWalker&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Detecting Antimicrobial Resistance from MALDI-TOF Mass Spectra with Statistical Guarantees Using Conformal Prediction&lt;/strong&gt; &lt;br /&gt;&#xA;Nina Corvelo Benz, Lucas Miranda, Dexiong Chen, Janko Sattler, Karsten Borgwardt,&#xA;&lt;em&gt;RECOMB&lt;/em&gt;, 2025 &lt;br /&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0031320324007180&#34;&gt;&lt;strong&gt;HTR-VT: Handwritten Text Recognition with Vision Transformer&lt;/strong&gt;&lt;/a&gt;  &lt;br /&gt;&#xA;Yuting Li, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Tinglong Tang, Xi Shen &lt;br /&gt;&#xA;&lt;em&gt;Pattern Recognition&lt;/em&gt;, 2025 &lt;br /&gt;&#xA;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0031320324007180&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://yutingli0606.github.io/HTR-VT/&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=8APPypS0yN&#34;&gt;&lt;strong&gt;On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Paolo Pellizzoni, Till Hendrik Schulz, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;NeurIPS&lt;/em&gt;, 2024 &lt;br /&gt;&#xA;&lt;a href=&#34;https://openreview.net/forum?id=8APPypS0yN&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/NodeIndividualizedGNNs&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://academic.oup.com/bioinformatics/article/40/Supplement_1/i501/7700883&#34;&gt;&lt;strong&gt;Biomarker Identification by Interpretable Maximum Mean Discrepancy&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Michael F Adamer, Sarah C Brüningk, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;ISMB&lt;/em&gt;, 2024 &lt;br /&gt;&#xA;&lt;a href=&#34;https://academic.oup.com/bioinformatics/article/40/Supplement_1/i501/7700883&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/spinoptmmd&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SURE_SUrvey_REcipes_for_building_reliable_and_robust_deep_networks_CVPR_2024_paper.pdf&#34;&gt;&lt;strong&gt;SURE: SUrvey REcipes for building reliable and robust deep networks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Yuting Li, Yingyi Chen, Xuanlong Yu, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Xi Shen &lt;br /&gt;&#xA;&lt;em&gt;CVPR&lt;/em&gt;, 2024 &lt;br /&gt;&#xA;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SURE_SUrvey_REcipes_for_building_reliable_and_robust_deep_networks_CVPR_2024_paper.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YutingLi0606/SURE&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=27vPcG4vKV&#34;&gt;&lt;strong&gt;ProteinShake: Building Datasets and Benchmarks for Deep Learning on Protein Structures&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Tim Kucera, Carlos Oliver, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;NeurIPS (Datasets and Benchmarks)&lt;/em&gt;, 2023 &lt;br /&gt;&#xA;&lt;a href=&#34;https://openreview.net/forum?id=27vPcG4vKV&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://proteinshake.ai/&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.07580&#34;&gt;&lt;strong&gt;Fisher Information Embedding for Node and Graph Learning&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Paolo Pellizzoni, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;ICML&lt;/em&gt;, 2023 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2305.07580&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/fisher_information_embedding&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=lUpjsrKItz4&#34;&gt;&lt;strong&gt;Unsupervised Manifold Alignment with Joint Multidimensional Scaling&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Bowen Fan, Carlos Oliver, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;ICLR&lt;/em&gt;, 2023 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2207.02968&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/JointMDS&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;poster/iclr2023.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.01008&#34;&gt;&lt;strong&gt;Approximate Network Motif Mining via Graph Learning&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Carlos Oliver, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Vincent Mallet, Pericles Philippopoulos, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;Preprint&lt;/em&gt;, 2022 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2206.01008&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/MotiFiesta&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fninf.2022.1032538/abstract&#34;&gt;&lt;strong&gt;Predicting in vitro single-neuron firing rates upon pharmacological perturbation using graph neural networks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Taehoon Kim, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Philipp Hornauer, Vishalini Emmenegger, Julian Bartram, Silvia Ronchi, Andreas R. Hierlemann, Manuel Schröter and Damian Roqueiro &lt;br /&gt;&#xA;&lt;em&gt;Frontiers in Neuroinformatics&lt;/em&gt;, 2022 &lt;br /&gt;&#xA;&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fninf.2022.1032538/abstract&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.03036&#34;&gt;&lt;strong&gt;Structure-Aware Transformer for Graph Representation Learning&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Leslie O&amp;rsquo;Bray, Karsten Borgwardt &lt;br /&gt;&#xA;&lt;em&gt;ICML&lt;/em&gt;, 2022 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2202.03036&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BorgwardtLab/SAT&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;poster/icml2022.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.05667&#34;&gt;&lt;strong&gt;GraphiT: Encoding Graph Structure in Transformers&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Grégoire Mialon, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Margot Selosse, Julien Mairal &lt;br /&gt;&#xA;&lt;em&gt;Preprint&lt;/em&gt;, 2021 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2106.05667&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/inria-thoth/GraphiT&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9366422&#34;&gt;&lt;strong&gt;Metamixup: Learning Adaptive Interpolation Policy of Mixup with Metalearning&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Zhijun Mai, Guosheng Hu, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Fumin Shen, Heng Tao Shen &lt;br /&gt;&#xA;&lt;em&gt;TNNLS&lt;/em&gt;, 2021 &lt;br /&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://hal.inria.fr/tel-03193220&#34;&gt;&lt;strong&gt;Structured Data Modeling with Deep Kernel Machines and Applications in Computational Biology&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt; &lt;br /&gt;&#xA;PhD thesis, Université Grenoble Alpes, 2020 &lt;br /&gt;&#xA;&lt;a href=&#34;https://hal.inria.fr/tel-03193220&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.12065&#34;&gt;&lt;strong&gt;A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Grégoire Mialon, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Alexandre d&amp;rsquo;Aspremont, Julien Mairal &lt;br /&gt;&#xA;&lt;em&gt;ICLR&lt;/em&gt;, 2021 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2006.12065&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/claying/OTK&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;poster/iclr2021.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.05189&#34;&gt;&lt;strong&gt;Convolutional Kernel Networks for Graph-Structured Data&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Laurent Jacob, Julien Mairal &lt;br /&gt;&#xA;In &lt;em&gt;ICML&lt;/em&gt;, 2020 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/2003.05189&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/claying/GCKN&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-16-20-00UTC-6029-convolutional_k.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.03200&#34;&gt;&lt;strong&gt;Recurrent Kernel Networks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Laurent Jacob, Julien Mairal &lt;br /&gt;&#xA;&lt;em&gt;NeurIPS&lt;/em&gt;, 2019 and accepted as an oral presentation in &lt;em&gt;MLCB&lt;/em&gt;, 2019 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/1906.03200&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/claying/RKN&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;poster/neurips2019.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.00363&#34;&gt;&lt;strong&gt;A Kernel Perspective for Regularizing Deep Neural Networks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;Alberto Bietti, Grégoire Mialon, &lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Julien Mairal &lt;br /&gt;&#xA;&lt;em&gt;ICML&lt;/em&gt;, 2019 &lt;br /&gt;&#xA;&lt;a href=&#34;https://arxiv.org/abs/1810.00363&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/albietz/kernel_reg&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;poster/icml2019.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1101/217257&#34;&gt;&lt;strong&gt;Biological Sequence Modeling with Convolutional Kernel Networks&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt;&#xA;&lt;strong&gt;Dexiong Chen&lt;/strong&gt;, Laurent Jacob, Julien Mairal &lt;br /&gt;&#xA;&lt;em&gt;Research in Computational Molecular Biology (RECOMB)&lt;/em&gt;, 2019 and &lt;em&gt;Bioinformatics&lt;/em&gt;, 2019 &lt;br /&gt;&#xA;&lt;a href=&#34;https://doi.org/10.1101/217257&#34;&gt;&lt;i class=&#34;fa-regular fa-file-pdf&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;https://gitlab.inria.fr/dchen/CKN-seq&#34;&gt;&lt;i class=&#34;fa-regular fa-file-code&#34;&gt;&lt;/i&gt;&lt;/a&gt; &lt;a href=&#34;poster/mlss2019.pdf&#34;&gt;&lt;i class=&#34;fa-regular fa-file-powerpoint&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Software</title>
      <link>https://claying.github.io/software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://claying.github.io/software/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/BorgwardtLab/proteinshake&#34;&gt;ProteinShake&lt;/a&gt; &lt;br /&gt;&#xA;ProteinShake provides one-liner imports of large scale, preprocessed protein structure tasks and datasets for various model types and frameworks.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/BorgwardtLab/SAT&#34;&gt;SAT&lt;/a&gt; &lt;br /&gt;&#xA;SAT provides a class of simple and flexible graph transformers built upon a new self-attention mechanism, which incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. SAT can leverage any existing GNN to extract the subgraph representation and systematically improve the peroformance relative to the base GNN.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/inria-thoth/GraphiT&#34;&gt;GraphiT&lt;/a&gt; &lt;br /&gt;&#xA;GraphiT is an instance of transformers designed for graph-structured data. It takes as input a graph seen as a set of its node features, and incorporate the graph structure via i) relative positional encoding using kernels on graphs and ii) encoding local substructures around each node, such as short paths, before adding it to the node features.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/claying/OTK&#34;&gt;OTK&lt;/a&gt; &lt;br /&gt;&#xA;Optimal transport kernel (OTK) is a kernel for feature aggregation. It allows to perform adaptive pooling (attention + pooling). Principally, it can be useful to model any data represented as sets of features (sequences, images, graphs etc.). In this implementation, it can be used as a module in neural networks, or alone as a kernel method.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/claying/GCKN&#34;&gt;GCKN&lt;/a&gt; &lt;br /&gt;&#xA;Graph convolutional kernel networks (GCKN) is a software package to model graph-structured data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/claying/RKN&#34;&gt;RKN&lt;/a&gt; &lt;br /&gt;&#xA;Recurrent kernel networks (RKN) is a software package to model biological sequences (DNA, protein etc.) with potentially gapped motifs.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/claying/CKN-Pytorch-image&#34;&gt;CKN-Pytorch-image&lt;/a&gt; &lt;br /&gt;&#xA;CKN-Pytorch-image is a software package to perform image classification with convolutional kernel networks.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://gitlab.inria.fr/dchen/CKN-seq&#34;&gt;CKN-seq&lt;/a&gt; &lt;br /&gt;&#xA;CKN-seq is a software package to model biological sequences (DNA, protein etc.) with convolutional kernel networks.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Talks</title>
      <link>https://claying.github.io/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://claying.github.io/talks/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;slides/thesis.pdf&#34;&gt;&lt;strong&gt;Structured Data Modeling with Deep Kernel Machines and Applications in Computational Biology&lt;/strong&gt;&lt;/a&gt;. PhD defense, Grenoble, 2020.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-16-20-00UTC-6029-convolutional_k.pdf&#34;&gt;&lt;strong&gt;Convolutional Kernel Networks for Graph-Structured Data&lt;/strong&gt;&lt;/a&gt;. ICML, Virtual, 2020.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;slides/mlcb2019.pdf&#34;&gt;&lt;strong&gt;Protein Fold Recognition with Recurrent Kernel Networks&lt;/strong&gt;&lt;/a&gt;. &lt;a href=&#34;https://sites.google.com/cs.washington.edu/mlcb&#34;&gt;MLCB&lt;/a&gt;, Vancouver, 2019.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Teaching</title>
      <link>https://claying.github.io/teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://claying.github.io/teaching/</guid>
      <description>&lt;h2 id=&#34;teaching-assistant-for-graduate-courses&#34;&gt;Teaching assistant for graduate courses&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;2022 Spring &lt;a href=&#34;https://bsse.ethz.ch/mlcb/education/data-mining2.html&#34;&gt;Data Mining II&lt;/a&gt;, CBB master - ETH Zürich.&lt;/li&gt;&#xA;&lt;li&gt;2018-2019 &lt;a href=&#34;http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/course/2019mva/index.html&#34;&gt;Kernel methods for machine learning&lt;/a&gt;, MVA master - ENS Paris Saclay.&lt;/li&gt;&#xA;&lt;li&gt;2018-2019 &lt;a href=&#34;https://thoth.inrialpes.fr/people/mairal/teaching/2018-2019/MSIAM/&#34;&gt;Advanced learning models&lt;/a&gt;, MSIAM master - Grenoble University.&lt;/li&gt;&#xA;&lt;li&gt;2017-2018 &lt;a href=&#34;http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/course/2018mva/index.html&#34;&gt;Kernel methods for machine learning&lt;/a&gt;, MVA master - ENS Paris Saclay.&lt;/li&gt;&#xA;&lt;li&gt;2017-2018 &lt;a href=&#34;https://thoth.inrialpes.fr/people/mairal/teaching/2017-2018/MSIAM/&#34;&gt;Advanced learning models&lt;/a&gt;, MSIAM master - Grenoble University.&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
